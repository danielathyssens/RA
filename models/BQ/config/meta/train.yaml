# @package _global_

# tensorboard --host localhost --port 8080 --logdir=./outputs

run_type: "train"
debug_lvl: 0  # 0 disables debugging and verbosity completely, >1 activates additional debugging functionality
global_seed: 1234
cuda: True
CPU_passMark:
GPU_passMark:

# global paths and logging
log_lvl: INFO
tb_logging: False
tb_log_path: 'logs/tb/'
log_path: 'logs/'
checkpoint_save_path: 'checkpoints/'
checkpoint_load_path:
  # None    # Path to load model parameters and optimizer state from#

test_cfg:
  data_file_path:          # data/test_data/cvrp/uniform/cvrp20_test_seed1234.pkl
  checkpoint_load_path:

val_dataset:             # Dataset file to use for validation
val_size: 10000 # 0 # 0             # Number of instances used for reporting validation performance
generator_args: {}


train_opts_cfg:
  run_name: 'Train_run_uchoa100'             # Name to identify the run
  use_cuda: True
  device: "cuda"
  graph_size: ${graph_size}              # The size of the problem graph
  val_size: 10000
  batch_size: 256       # Number of instances per batch during training
  epoch_size: 1280000         # Number of instances per epoch during training
  epoch_start: 0
  n_epochs: 100
  checkpoint_epochs: 1
  lr_model: 0.0001              # Set the learning rate for the actor network
  lr_critic: 0.0001             # Set the learning rate for the critic network
  lr_decay: 1.0               # Learning rate decay per epoch
  eval_only: False            # Set this value to only evaluate model
  max_grad_norm: 1.0          # Max. L2 norm for grad-clipping,default 1.0 (0 to disable clipping)
  exp_beta: 0.8               # Exponential moving average baseline decay (default 0.8)
  baseline: rollout           # Baseline: 'rollout','critic','exponential'. Default: no baseline.
  bl_alpha: 0.05              # Significance in the t-test for updating rollout baseline
  bl_warmup_epochs: 1         # Number epochs to warmup the baseline, default None = 1 (rollout)
  eval_batch_size: 1024       # Batch size to use during (baseline) evaluation
  checkpoint_encoder: False   # Set to decrease memory usage by checkpointing encoder
  data_distribution:          # Data distribution to use during training (depends on problem)
  no_tensorboard: True
  no_progress_bar: False      # Disable progress bar
  log_step: 50                # Log info every log_step steps'
  n_EG: ${model_cfg.model_args.n_EG}
  kl_loss: 0.0
