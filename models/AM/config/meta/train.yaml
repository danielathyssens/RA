# @package _global_

# tensorboard --host localhost --port 8080 --logdir=./outputs

run_type: "train"
debug_lvl: 0  # 0 disables debugging and verbosity completely, >1 activates additional debugging functionality
global_seed: 1234
cuda: True

val_size: 10000                # Number of instances used for reporting validation performance
val_dataset: # None            # Dataset file to use for validation
generator_args: {}


test_cfg:
  data_file_path: # None         # data/test_data/cvrp/uniform/cvrp20_test_seed1234.pkl
  checkpoint_load_path: #None
  saved_res_dir: #None

train_opts_cfg:
  run_name: 'train_run_uchoa100'       # Name to identify the run
  graph_size: ${graph_size}              # The size of the problem graph
  batch_size: 512             # Number of instances per batch during training
  epoch_size: 1280000 # 0 #0         # Number of instances per epoch during training
  n_epochs: 100               # The number of epochs to train
  epoch_start: 0
  checkpoint_epochs: 1        # Save checkpoint every n epochs (default 1), 0 to save no checkpoints
  lr_model: 1e-4              # Set the learning rate for the actor network
  lr_critic: 1e-4             # Set the learning rate for the critic network
  lr_decay: 1.0               # Learning rate decay per epoch
  eval_only: False            # Set this value to only evaluate model
  seed: 1234                  # Random seed to use
  max_grad_norm: 1.0          # Max. L2 norm for grad-clipping,default 1.0 (0 to disable clipping)
  exp_beta: 0.8               # Exponential moving average baseline decay (default 0.8)
  baseline: rollout           # Baseline: 'rollout','critic','exponential'. Default: no baseline.
  bl_alpha: 0.05              # Significance in the t-test for updating rollout baseline
  bl_warmup_epochs: 1         # Number epochs to warmup the baseline, default None = 1 (rollout)
  eval_batch_size: 1024       # Batch size to use during (baseline) evaluation
  checkpoint_encoder: False   # Set to decrease memory usage by checkpointing encoder
  shrink_size: None           # Shrink bs if at least this many instances in batch are finished
  data_distribution: None     # Data distribution to use during training (depends on problem)
  no_progress_bar: False      # Disable progress bar
  log_step: 50                # Log info every log_step steps'
  no_tensorboard: True        # Disable logging TensorBoard files

#    time_limit: ${test_cfg.time_limit} #4
# global paths and logging
log_lvl: INFO
tb_logging: False
tb_log_path: 'logs/tb/'
log_path: 'logs/'
checkpoint_save_path: 'checkpoints/'


